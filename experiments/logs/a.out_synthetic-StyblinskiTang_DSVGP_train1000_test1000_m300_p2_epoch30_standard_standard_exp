wandb: Currently logged in as: jimmypotato (use `wandb login --relogin` to force relogin)
wandb: Tracking run with wandb version 0.10.25
wandb: Syncing run StyblinskiTang_DSVGP_ntrain1000_m300_p2_epochs30_standard_standard_expTEST_ntest1000
wandb:  View project at https://wandb.ai/jimmypotato/DSVGP
wandb:  View run at https://wandb.ai/jimmypotato/DSVGP/runs/zko9z0vq
wandb: Run data is saved locally in C:\Users\Leo Huang\Desktop\GP-Derivatives-Variational-Inference\experiments\wandb\run-20210409_154947-zko9z0vq
wandb: Run `wandb offline` to turn off syncing.

Experiment settings:
{'log_dir': './logs/', 'data_dir': './data/', 'save_model': False, 'watch_model': True, 'save_results': True, 'exp_name': 'TEST', 'dataset': 'synthetic-StyblinskiTang', 'model': 'DSVGP', 'variational_strategy': 'standard', 'variational_distribution': 'standard', 'n_train': 1000, 'n_test': 1000, 'num_inducing': 300, 'num_directions': 2, 'num_epochs': 30, 'batch_size': 50, 'lr': 0.01, 'lr_ngd': 0.1, 'num_contour_quad': 15, 'lr_sched': 'step_lr', 'seed': 0, 'derivative': True}



Start Experiment: StyblinskiTang_DSVGP_ntrain1000_m300_p2_epochs30_standard_standard_expTEST_ntest1000

---D-SVGP---
Variational distribution: standard, Variational strategy: standard
Start training with 1000 trainig data of dim 2
VI setups: 300 inducing points, 2 inducing directions
Epoch: 0; Step: 0, loss: 82.87290553109548, nll: 1.4200781883449654
Epoch: 0; Step: 10, loss: 74.69641978479088, nll: 1.5610933376472038
Epoch: 1; Step: 0, loss: 61.568967219615004, nll: 1.3949745193645482
Epoch: 1; Step: 10, loss: 62.184795674070216, nll: 1.505403604432322
Epoch: 2; Step: 0, loss: 58.97289767108131, nll: 1.503355503888783
Epoch: 2; Step: 10, loss: 35.0353900666034, nll: 1.3106167573083618
Epoch: 3; Step: 0, loss: 31.613343313186814, nll: 1.2345372674285215
Epoch: 3; Step: 10, loss: 35.713173985454866, nll: 1.256512014115435
Epoch: 4; Step: 0, loss: 29.768692756518877, nll: 1.292944377408233
Epoch: 4; Step: 10, loss: 26.303827252080175, nll: 1.3027309301974284
Epoch: 5; Step: 0, loss: 22.566256100357407, nll: 1.3039860286058387
Epoch: 5; Step: 10, loss: 15.27134230605165, nll: 1.205197360452758
Epoch: 6; Step: 0, loss: 9.142562474224999, nll: 1.2382763598434858
Epoch: 6; Step: 10, loss: 9.631202767880588, nll: 1.1584827491189145
Epoch: 7; Step: 0, loss: 9.389085839027201, nll: 1.2121227885481052
Epoch: 7; Step: 10, loss: 5.810460382666826, nll: 1.1251419195581043
Epoch: 8; Step: 0, loss: 4.161175768431587, nll: 1.1638874295531247
Epoch: 8; Step: 10, loss: 6.427092196819851, nll: 1.2292194111771966
Epoch: 9; Step: 0, loss: 3.942416080931952, nll: 1.2255348275447924
Epoch: 9; Step: 10, loss: 3.4925045068495004, nll: 1.2310624901387635
Epoch: 10; Step: 0, loss: 2.5387245237058833, nll: 1.2764221643484523
Epoch: 10; Step: 10, loss: 3.5049598525068784, nll: 1.2871838733289018
Epoch: 11; Step: 0, loss: 3.116450462263213, nll: 1.2711229387319998
Epoch: 11; Step: 10, loss: 2.805327005629515, nll: 1.2621278045049769
Epoch: 12; Step: 0, loss: 2.844700584227468, nll: 1.2652875875391874
Epoch: 12; Step: 10, loss: 2.861114676246711, nll: 1.2569873728609948
Epoch: 13; Step: 0, loss: 3.702511540636909, nll: 1.2880621001899675
Epoch: 13; Step: 10, loss: 2.6973363226154006, nll: 1.2402556116115802
Epoch: 14; Step: 0, loss: 2.8641550973707965, nll: 1.2468236618173147
Epoch: 14; Step: 10, loss: 2.510487838290113, nll: 1.2320965240196793
Epoch: 15; Step: 0, loss: 3.141985006361148, nll: 1.2789590447609063
Epoch: 15; Step: 10, loss: 2.751167013525208, nll: 1.2509677314815975
Epoch: 16; Step: 0, loss: 2.4975711662813964, nll: 1.2432608781942753
Epoch: 16; Step: 10, loss: 3.55848492691183, nll: 1.2752823038468784
Epoch: 17; Step: 0, loss: 2.254295551089392, nll: 1.2229262680559159
Epoch: 17; Step: 10, loss: 2.6782797943005825, nll: 1.248039268365272
Epoch: 18; Step: 0, loss: 4.535517619135917, nll: 1.2783331484393647
Epoch: 18; Step: 10, loss: 2.1824282799423846, nll: 1.2748650440048988
Epoch: 19; Step: 0, loss: 2.6908629340824604, nll: 1.2156003717491333
Epoch: 19; Step: 10, loss: 2.699426604502149, nll: 1.2419729123678511
Epoch: 20; Step: 0, loss: 2.6524512261220266, nll: 1.2499479287346384
Epoch: 20; Step: 10, loss: 2.4870597930413116, nll: 1.2365135396425324
Epoch: 21; Step: 0, loss: 2.2643891542734975, nll: 1.2591353310063056
Epoch: 21; Step: 10, loss: 2.2571605168905613, nll: 1.2478186896606531
Epoch: 22; Step: 0, loss: 2.527443027034971, nll: 1.2548525052380877
Epoch: 22; Step: 10, loss: 2.9200106892915816, nll: 1.2733409361665673
Epoch: 23; Step: 0, loss: 3.2408995265092675, nll: 1.2316808286251935
Epoch: 23; Step: 10, loss: 2.798113117452804, nll: 1.2354281921688566
Epoch: 24; Step: 0, loss: 2.577240026521545, nll: 1.2343652416397286
Epoch: 24; Step: 10, loss: 2.5308229755222813, nll: 1.253270285931659
Epoch: 25; Step: 0, loss: 3.0633168257930623, nll: 1.2680518192766492
Epoch: 25; Step: 10, loss: 2.2442332059816046, nll: 1.254058750713055
Epoch: 26; Step: 0, loss: 4.336103834047356, nll: 1.291288580282147
Epoch: 26; Step: 10, loss: 2.2961961408245446, nll: 1.2677387804007325
Epoch: 27; Step: 0, loss: 2.361988566574833, nll: 1.2293015494880037
Epoch: 27; Step: 10, loss: 2.414207649082676, nll: 1.2520883817015467
Epoch: 28; Step: 0, loss: 3.7871486218888, nll: 1.265027666045622
Epoch: 28; Step: 10, loss: 2.3191458656856443, nll: 1.2238450618053975
Epoch: 29; Step: 0, loss: 3.0934530813704972, nll: 1.2354873542392646
Epoch: 29; Step: 10, loss: 2.8675698931503297, nll: 1.2616762368800614
Done! loss: 2.731750449763528

Done Training!
Done Testing!
At 1000 testing points, MSE: 5.4907e-01, RMSE: 7.4099e-01, MAE: 7.2605e-01, nll: 1.2482e+00.
Training time: 801.13 sec, testing time: 8.43 sec
Traceback (most recent call last):
  File "exp_script.py", line 249, in <module>
    main(**vars(args))
  File "exp_script.py", line 207, in main
    pickle.dump(summary,open(f"./postprocess/exp_res/{expname_test}.pkl","wb"))
FileNotFoundError: [Errno 2] No such file or directory: './postprocess/exp_res/StyblinskiTang_DSVGP_ntrain1000_m300_p2_epochs30_standard_standard_expTEST_ntest1000.pkl'

wandb: Waiting for W&B process to finish, PID 7924
wandb: Program failed with code 1.  Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)
wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)
wandb: | 0.00MB of 0.01MB uploaded (0.00MB deduped)
wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)
wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)
wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)
wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)
wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)
wandb:                                                                                
wandb: Find user logs for this run at: C:\Users\Leo Huang\Desktop\GP-Derivatives-Variational-Inference\experiments\wandb\run-20210409_154947-zko9z0vq\logs\debug.log
wandb: Find internal logs for this run at: C:\Users\Leo Huang\Desktop\GP-Derivatives-Variational-Inference\experiments\wandb\run-20210409_154947-zko9z0vq\logs\debug-internal.log
wandb: Run summary:
wandb:         loss 2.73175
wandb:     _runtime 806
wandb:   _timestamp 1617998594
wandb:        _step 599
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced StyblinskiTang_DSVGP_ntrain1000_m300_p2_epochs30_standard_standard_expTEST_ntest1000: https://wandb.ai/jimmypotato/DSVGP/runs/zko9z0vq
