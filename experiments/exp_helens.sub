#!/bin/bash
#SBATCH -J  basic                 # Job name
#SBATCH -o ../slurm_output/job_%j.out    # Name of stdout output file(%j expands to jobId)
#SBATCH -e ../slurm_output/job_%j.err    # Name of stderr output file(%j expands to jobId)
#SBATCH --get-user-env                     # Tells sbatch to retrieve the users login environment
#SBATCH -N 1                               # Total number of nodes requested
#SBATCH -n 8                               # Total number of cores requested
#SBATCH --mem=32G                        # Total amount of (real) memory requested (per node)
#SBATCH -t 168:00:00                       # Time limit (hh:mm:ss)
#SBATCH --partition=default_gpu            # Request partition for resource 
#SBATCH --gres=gpu:1                       # Specify a list of generic consumable resources (per node)

set -e
. /home/xz584/anaconda3/etc/profile.d/conda.sh
conda activate DSVGP

# dataset="synthetic-Branin" # synthetic/real - dataset name 
dataset="real-helens"

# exp setups
# fix some setups for this dataset
# n_train=12636
# n_test=1404
n_train=900
n_test=13140
batch_size=512
watch_model=True
# read other arguments from command line when sbatch this job
model=${1}
variational_strategy=${2}
variational_distribution=${3}
num_inducing=${4}
num_directions=${5}
num_epochs=${6}
exp_name=${7}
lr=${8}
lr_ngd=${9}
num_contour_quad=${10}
seed=${11}

# compare different methods, comment out the chunk if not comparing with this method
# find runlogs in logs folder
sh ./exp_setup.sh ${dataset} ${variational_strategy} ${variational_distribution}\
                  ${n_train} ${n_test} ${num_inducing}\
                  ${num_directions} ${num_epochs} ${batch_size} ${model}\
                  ${lr} ${lr_ngd} ${num_contour_quad} ${watch_model} ${exp_name} ${seed}
